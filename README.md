# ğŸ¤– KinderChatbot

**KinderChatbot** is an AI-powered chatbot tailored for kindergarten-teacher-style conversational use cases. It combines **Supervised Fine-Tuning (SFT)** and **Reward Modeling** to achive **Reinforcement Learning from Human Feedback (RLHF)**. Lightweight **LoRA adapters** enable efficient fine-tuning on the **attention layers** of pretrained model, while a custom reward model trained with human preferences intelligently selects the best reply.

---

## âœ¨ Features

- **Supervised Fine-Tuning (SFT):** Fine-tune a base language model using LoRA adapters for efficient and modular training.
- **Reward Model (RLHF):** Train a reward model with human preferences to score and rerank multiple generated responses.
- **Data-free Knowledge Distillation:** Generate synthetic dialogues as training data, and review by humans.
- **Modular Codebase:** Clean, extensible architecture for dataset generation, SFT training, reward modeling, and end-to-end inference.
- **Gradio Interface (bonus):** Interactive and user-friendly web UI deployed on HuggingFace Space for real-time chatting.

---

## ğŸ—‚ï¸ Project Structure

```
.env
.gitignore
main.py
README.md
requirements.txt

Dataset/
â””â”€â”€ generate_data.py

models/
â”œâ”€â”€ reward/
â””â”€â”€ sft/

utils/
â”œâ”€â”€ reward.py
â”œâ”€â”€ RLHF.py
â””â”€â”€ sft.py
```

- `main.py`: Entry point for running the chatbot.
- `requirements.txt`: Python dependencies.
- `Dataset/`: Script for generating synthetic training data and directory for storing the generated data.
- `models/`: Directory for storing sft and reward models with their checkpoints.
- `utils/`: Core scripts for sft training, reward scoring training, and RLHF-based reranking.

---

## ğŸš€ Getting Started

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Download Model Weights

Model weights and tokenizer files are **not included** in this repository. Please download them from my HuggingFace: [LoRA fine-tuned SFT model](https://huggingface.co/Miao025/Qwen-KinderChatbot-LoRA), [Reward model](https://huggingface.co/Miao025/Qwen-KinderChatbot-Reward), and place them into the correct directories:

- LoRA fine-tuned SFT model â†’ `models/sft/lora_adapter/`
- Reward model â†’ `models/reward/reward_model/`

---

### 3. Try the Chatbot

Run the following command to start the chatbot interface:

```bash
python main.py
```
You can also interact with the chatbot [online](https://huggingface.co/spaces/Miao025/qwen-kinderchatbot).

---

## ğŸƒâ€ Training
- **Supervised Fine-Tuning (SFT):** See `sft.py` for fine-tuning the base model (Qwen1.5-1.8B) with LoRA adapters.
- **Reward Model Training:** See `reward.py` for full training the base reward model (DistilbBERT).
- **Ranker:** See `RLHF.py` for calling fine-tuned models, using reward model to rank, and choose the top answer.
- **Data Generation** See `generate_data.py` to create and save the training data for sft and reward.

---
## ğŸ‘©ğŸ» Contributor
- Miao

---
## ğŸ“ƒ References
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)
- [LoRA](https://huggingface.co/docs/diffusers/training/lora)
- [DistilBERT](https://huggingface.co/onnxport/distilbert-base-uncased-onnx)
- [Gradio](https://pypi.org/project/gradio/2.9b50/)